{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac8e009",
   "metadata": {},
   "source": [
    "# Jupyter AI\n",
    "Jupyter AI is a JupyterLab extension that provides a friendly user interface between users and AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6604c592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8071b73",
   "metadata": {},
   "source": [
    "#### Loadingthe IPython extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52ee791",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_ai_magics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f551328",
   "metadata": {},
   "source": [
    "#### Getting help with the %%ai command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b65079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: %%ai [OPTIONS] MODEL_ID\n",
      "\n",
      "  Invokes a language model identified by MODEL_ID, with the prompt being\n",
      "  contained in all lines after the first. Both local model IDs and global\n",
      "  model IDs (with the provider ID explicitly prefixed, followed by a colon)\n",
      "  are accepted.\n",
      "\n",
      "  To view available language models, please run `%ai list`.\n",
      "\n",
      "Options:\n",
      "  -f, --format [code|html|image|json|markdown|math|md|text]\n",
      "                                  IPython display to use when rendering\n",
      "                                  output. [default=\"markdown\"]\n",
      "  -r, --reset                     Clears the conversation transcript used when\n",
      "                                  interacting with an OpenAI chat model\n",
      "                                  provider. Does nothing with other providers.\n",
      "  -n, --region-name TEXT          AWS region name, e.g. 'us-east-1'. Required\n",
      "                                  for SageMaker provider; does nothing with\n",
      "                                  other providers.\n",
      "  -q, --request-schema TEXT       The JSON object the endpoint expects, with\n",
      "                                  the prompt being substituted into any value\n",
      "                                  that matches the string literal '<prompt>'.\n",
      "                                  Required for SageMaker provider; does\n",
      "                                  nothing with other providers.\n",
      "  -p, --response-path TEXT        A JSONPath string that retrieves the\n",
      "                                  language model's output from the endpoint's\n",
      "                                  JSON response. Required for SageMaker\n",
      "                                  provider; does nothing with other providers.\n",
      "  -m, --model-parameters TEXT     A JSON value that specifies extra values\n",
      "                                  that will be passed to the model. The\n",
      "                                  accepted value parsed to a dict, unpacked\n",
      "                                  and passed as-is to the provider class.\n",
      "  --help                          Show this message and exit.\n",
      "------------------------------------------------------------------------------\n",
      "Usage: %ai [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Invokes a subcommand.\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  delete    Delete an alias. See `%ai delete --help` for options.\n",
      "  error     Explains the most recent error.\n",
      "  help      Show this message and exit.\n",
      "  list      List language models. See `%ai list --help` for options.\n",
      "  register  Register a new alias. See `%ai register --help` for options.\n",
      "  update    Update the target of an alias. See `%ai update --help` for\n",
      "            options.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%ai help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cc810ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: %ai list [OPTIONS] [PROVIDER_ID]\n",
      "\n",
      "  List language models, optionally scoped to PROVIDER_ID.\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "%ai list --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61cc4b",
   "metadata": {},
   "source": [
    "#### Listing all providers and models available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c6bc2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `ai21:j1-large`, `ai21:j1-grande`, `ai21:j1-jumbo`, `ai21:j1-grande-instruct`, `ai21:j2-large`, `ai21:j2-grande`, `ai21:j2-jumbo`, `ai21:j2-grande-instruct`, `ai21:j2-jumbo-instruct` |\n",
       "| `bedrock` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `bedrock:amazon.titan-text-express-v1`, `bedrock:ai21.j2-ultra-v1`, `bedrock:ai21.j2-mid-v1`, `bedrock:cohere.command-light-text-v14`, `bedrock:cohere.command-text-v14`, `bedrock:meta.llama2-13b-chat-v1`, `bedrock:meta.llama2-70b-chat-v1` |\n",
       "| `bedrock-chat` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `bedrock-chat:anthropic.claude-v1`, `bedrock-chat:anthropic.claude-v2`, `bedrock-chat:anthropic.claude-v2:1`, `bedrock-chat:anthropic.claude-instant-v1` |\n",
       "| `anthropic` | `ANTHROPIC_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `anthropic:claude-v1`, `anthropic:claude-v1.0`, `anthropic:claude-v1.2`, `anthropic:claude-2`, `anthropic:claude-2.0`, `anthropic:claude-instant-v1`, `anthropic:claude-instant-v1.0`, `anthropic:claude-instant-v1.2` |\n",
       "| `anthropic-chat` | `ANTHROPIC_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `anthropic-chat:claude-v1`, `anthropic-chat:claude-v1.0`, `anthropic-chat:claude-v1.2`, `anthropic-chat:claude-2`, `anthropic-chat:claude-2.0`, `anthropic-chat:claude-instant-v1`, `anthropic-chat:claude-instant-v1.0`, `anthropic-chat:claude-instant-v1.2` |\n",
       "| `azure-chat-openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | This provider does not define a list of models. |\n",
       "| `cohere` | `COHERE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `cohere:medium`, `cohere:xlarge` |\n",
       "| `gpt4all` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `gpt4all:ggml-gpt4all-j-v1.2-jazzy`, `gpt4all:ggml-gpt4all-j-v1.3-groovy`, `gpt4all:ggml-gpt4all-l13b-snoozy`, `gpt4all:mistral-7b-openorca.Q4_0`, `gpt4all:mistral-7b-instruct-v0.1.Q4_0`, `gpt4all:gpt4all-falcon-q4_0`, `gpt4all:wizardlm-13b-v1.2.Q4_0`, `gpt4all:nous-hermes-llama2-13b.Q4_0`, `gpt4all:gpt4all-13b-snoozy-q4_0`, `gpt4all:mpt-7b-chat-merges-q4_0`, `gpt4all:orca-mini-3b-gguf2-q4_0`, `gpt4all:starcoder-q4_0`, `gpt4all:rift-coder-v0-7b-q4_0`, `gpt4all:em_german_mistral_v01.Q4_0` |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai:text-davinci-003`, `openai:text-davinci-002`, `openai:text-curie-001`, `openai:text-babbage-001`, `openai:text-ada-001`, `openai:davinci`, `openai:curie`, `openai:babbage`, `openai:ada` |\n",
       "| `openai-chat` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai-chat:gpt-3.5-turbo`, `openai-chat:gpt-3.5-turbo-16k`, `openai-chat:gpt-3.5-turbo-0301`, `openai-chat:gpt-3.5-turbo-0613`, `openai-chat:gpt-3.5-turbo-16k-0613`, `openai-chat:gpt-4`, `openai-chat:gpt-4-0314`, `openai-chat:gpt-4-0613`, `openai-chat:gpt-4-32k`, `openai-chat:gpt-4-32k-0314`, `openai-chat:gpt-4-32k-0613`, `openai-chat:gpt-4-1106-preview` |\n",
       "| `openai-chat-new` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai-chat-new:gpt-3.5-turbo`, `openai-chat-new:gpt-3.5-turbo-16k`, `openai-chat-new:gpt-3.5-turbo-0301`, `openai-chat-new:gpt-3.5-turbo-0613`, `openai-chat-new:gpt-3.5-turbo-16k-0613`, `openai-chat-new:gpt-4`, `openai-chat-new:gpt-4-0314`, `openai-chat-new:gpt-4-0613`, `openai-chat-new:gpt-4-32k`, `openai-chat-new:gpt-4-32k-0314`, `openai-chat-new:gpt-4-32k-0613`, `openai-chat-new:gpt-4-1106-preview` |\n",
       "| `qianfan` | `QIANFAN_AK`, `QIANFAN_SK` | <abbr title=\"You have not set all of these environment variables, so you cannot use this provider's models.\">❌</abbr> | `qianfan:ERNIE-Bot`, `qianfan:ERNIE-Bot-4` |\n",
       "| `sagemaker-endpoint` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints). |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:text-davinci-003` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n",
       "| `ernie-bot` | `qianfan:ERNIE-Bot` |\n",
       "| `ernie-bot-4` | `qianfan:ERNIE-Bot-4` |\n",
       "| `titan` | `bedrock:amazon.titan-tg1-large` |\n"
      ],
      "text/plain": [
       "ai21\n",
       "Requires environment variable: AI21_API_KEY (not set)\n",
       "* ai21:j1-large\n",
       "* ai21:j1-grande\n",
       "* ai21:j1-jumbo\n",
       "* ai21:j1-grande-instruct\n",
       "* ai21:j2-large\n",
       "* ai21:j2-grande\n",
       "* ai21:j2-jumbo\n",
       "* ai21:j2-grande-instruct\n",
       "* ai21:j2-jumbo-instruct\n",
       "\n",
       "bedrock\n",
       "* bedrock:amazon.titan-text-express-v1\n",
       "* bedrock:ai21.j2-ultra-v1\n",
       "* bedrock:ai21.j2-mid-v1\n",
       "* bedrock:cohere.command-light-text-v14\n",
       "* bedrock:cohere.command-text-v14\n",
       "* bedrock:meta.llama2-13b-chat-v1\n",
       "* bedrock:meta.llama2-70b-chat-v1\n",
       "\n",
       "bedrock-chat\n",
       "* bedrock-chat:anthropic.claude-v1\n",
       "* bedrock-chat:anthropic.claude-v2\n",
       "* bedrock-chat:anthropic.claude-v2:1\n",
       "* bedrock-chat:anthropic.claude-instant-v1\n",
       "\n",
       "anthropic\n",
       "Requires environment variable: ANTHROPIC_API_KEY (not set)\n",
       "* anthropic:claude-v1\n",
       "* anthropic:claude-v1.0\n",
       "* anthropic:claude-v1.2\n",
       "* anthropic:claude-2\n",
       "* anthropic:claude-2.0\n",
       "* anthropic:claude-instant-v1\n",
       "* anthropic:claude-instant-v1.0\n",
       "* anthropic:claude-instant-v1.2\n",
       "\n",
       "anthropic-chat\n",
       "Requires environment variable: ANTHROPIC_API_KEY (not set)\n",
       "* anthropic-chat:claude-v1\n",
       "* anthropic-chat:claude-v1.0\n",
       "* anthropic-chat:claude-v1.2\n",
       "* anthropic-chat:claude-2\n",
       "* anthropic-chat:claude-2.0\n",
       "* anthropic-chat:claude-instant-v1\n",
       "* anthropic-chat:claude-instant-v1.0\n",
       "* anthropic-chat:claude-instant-v1.2\n",
       "\n",
       "azure-chat-openai\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* This provider does not define a list of models.\n",
       "\n",
       "cohere\n",
       "Requires environment variable: COHERE_API_KEY (not set)\n",
       "* cohere:medium\n",
       "* cohere:xlarge\n",
       "\n",
       "gpt4all\n",
       "* gpt4all:ggml-gpt4all-j-v1.2-jazzy\n",
       "* gpt4all:ggml-gpt4all-j-v1.3-groovy\n",
       "* gpt4all:ggml-gpt4all-l13b-snoozy\n",
       "* gpt4all:mistral-7b-openorca.Q4_0\n",
       "* gpt4all:mistral-7b-instruct-v0.1.Q4_0\n",
       "* gpt4all:gpt4all-falcon-q4_0\n",
       "* gpt4all:wizardlm-13b-v1.2.Q4_0\n",
       "* gpt4all:nous-hermes-llama2-13b.Q4_0\n",
       "* gpt4all:gpt4all-13b-snoozy-q4_0\n",
       "* gpt4all:mpt-7b-chat-merges-q4_0\n",
       "* gpt4all:orca-mini-3b-gguf2-q4_0\n",
       "* gpt4all:starcoder-q4_0\n",
       "* gpt4all:rift-coder-v0-7b-q4_0\n",
       "* gpt4all:em_german_mistral_v01.Q4_0\n",
       "\n",
       "huggingface_hub\n",
       "Requires environment variable: HUGGINGFACEHUB_API_TOKEN (set)\n",
       "* See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`.\n",
       "\n",
       "openai\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai:text-davinci-003\n",
       "* openai:text-davinci-002\n",
       "* openai:text-curie-001\n",
       "* openai:text-babbage-001\n",
       "* openai:text-ada-001\n",
       "* openai:davinci\n",
       "* openai:curie\n",
       "* openai:babbage\n",
       "* openai:ada\n",
       "\n",
       "openai-chat\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai-chat:gpt-3.5-turbo\n",
       "* openai-chat:gpt-3.5-turbo-16k\n",
       "* openai-chat:gpt-3.5-turbo-0301\n",
       "* openai-chat:gpt-3.5-turbo-0613\n",
       "* openai-chat:gpt-3.5-turbo-16k-0613\n",
       "* openai-chat:gpt-4\n",
       "* openai-chat:gpt-4-0314\n",
       "* openai-chat:gpt-4-0613\n",
       "* openai-chat:gpt-4-32k\n",
       "* openai-chat:gpt-4-32k-0314\n",
       "* openai-chat:gpt-4-32k-0613\n",
       "* openai-chat:gpt-4-1106-preview\n",
       "\n",
       "openai-chat-new\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai-chat-new:gpt-3.5-turbo\n",
       "* openai-chat-new:gpt-3.5-turbo-16k\n",
       "* openai-chat-new:gpt-3.5-turbo-0301\n",
       "* openai-chat-new:gpt-3.5-turbo-0613\n",
       "* openai-chat-new:gpt-3.5-turbo-16k-0613\n",
       "* openai-chat-new:gpt-4\n",
       "* openai-chat-new:gpt-4-0314\n",
       "* openai-chat-new:gpt-4-0613\n",
       "* openai-chat-new:gpt-4-32k\n",
       "* openai-chat-new:gpt-4-32k-0314\n",
       "* openai-chat-new:gpt-4-32k-0613\n",
       "* openai-chat-new:gpt-4-1106-preview\n",
       "\n",
       "qianfan\n",
       "Requires environment variables: QIANFAN_AK (not set), QIANFAN_SK (not set)\n",
       "* qianfan:ERNIE-Bot\n",
       "* qianfan:ERNIE-Bot-4\n",
       "\n",
       "sagemaker-endpoint\n",
       "* Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints).\n",
       "\n",
       "\n",
       "Aliases and custom commands:\n",
       "gpt2 - huggingface_hub:gpt2\n",
       "gpt3 - openai:text-davinci-003\n",
       "chatgpt - openai-chat:gpt-3.5-turbo\n",
       "gpt4 - openai-chat:gpt-4\n",
       "ernie-bot - qianfan:ERNIE-Bot\n",
       "ernie-bot-4 - qianfan:ERNIE-Bot-4\n",
       "titan - bedrock:amazon.titan-tg1-large\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb6b1cb",
   "metadata": {},
   "source": [
    "#### Listing all models offered by a specific provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada098fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai:text-davinci-003`, `openai:text-davinci-002`, `openai:text-curie-001`, `openai:text-babbage-001`, `openai:text-ada-001`, `openai:davinci`, `openai:curie`, `openai:babbage`, `openai:ada` |\n"
      ],
      "text/plain": [
       "openai\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai:text-davinci-003\n",
       "* openai:text-davinci-002\n",
       "* openai:text-curie-001\n",
       "* openai:text-babbage-001\n",
       "* openai:text-ada-001\n",
       "* openai:davinci\n",
       "* openai:curie\n",
       "* openai:babbage\n",
       "* openai:ada\n",
       "\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dac14c",
   "metadata": {},
   "source": [
    "#### Changing the format of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e31899d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjaydasgupta/.pyenv/versions/pdf_web/lib/python3.10/site-packages/langchain_community/llms/openai.py:1057: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \n",
       "\\frac{\\partial u}{\\partial t} = \\alpha \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right)\n",
       "$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "text/latex": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt -f math\n",
    "Generate the 2D heat equation in LaTeX surrounded by `$$`. Do not include an explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf6110",
   "metadata": {},
   "source": [
    "#### Making a request to gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e46d912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjaydasgupta/.pyenv/versions/pdf_web/lib/python3.10/site-packages/langchain_community/llms/openai.py:1057: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Lists, tuples, and sets are three different types of data structures in Python, each with unique characteristics:\n",
       "\n",
       "1. Lists:  \n",
       "   - Denoted by square brackets [ ].\n",
       "   - Mutable, meaning their elements can be modified after creation.\n",
       "   - Elements can be of different data types.\n",
       "   - Allows duplicate elements.\n",
       "   - Supports indexing and slicing.\n",
       "\n",
       "2. Tuples:\n",
       "   - Denoted by parentheses ( ).\n",
       "   - Immutable, meaning their elements cannot be modified after creation.\n",
       "   - Elements can be of different data types.\n",
       "   - Allows duplicate elements.\n",
       "   - Supports indexing and slicing.\n",
       "\n",
       "3. Sets:\n",
       "   - Denoted by curly braces { } or using the set() function.\n",
       "   - Mutable, meaning their elements can be modified after creation.\n",
       "   - Elements are unique and unordered (no specific order).\n",
       "   - Elements can be of different data types.\n",
       "   - Does not support indexing or slicing.\n",
       "   - Provides set operations like union, intersection, difference, and more."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai openai-chat:gpt-3.5-turbo\n",
    "lists vs. tuples vs. sets in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b926252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjaydasgupta/.pyenv/versions/pdf_web/lib/python3.10/site-packages/langchain_community/llms/openai.py:1057: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "To sort a list in Python, you can use the `sort()` method or the `sorted()` function. Here are the steps for each method:\n",
       "\n",
       "1. `sort()` method:\n",
       "   - This method sorts the list in-place, meaning it modifies the original list.\n",
       "   - Call the `sort()` method on the list you want to sort.\n",
       "   - Optionally, you can pass the argument `reverse=True` inside `sort()` to sort the list in descending order. By default, `reverse=False` sorts the list in ascending order.\n",
       "\n",
       "   Example:\n",
       "   ```python\n",
       "   my_list = [4, 2, 1, 3]\n",
       "   my_list.sort()\n",
       "   print(my_list)  # Output: [1, 2, 3, 4]\n",
       "   ```\n",
       "\n",
       "2. `sorted()` function:\n",
       "   - This function returns a new sorted list and does not modify the original list.\n",
       "   - Call the `sorted()` function, passing the list as an argument.\n",
       "   - Optionally, you can pass the argument `reverse=True` inside `sorted()` to sort the list in descending order. By default, `reverse=False` sorts the list in ascending order.\n",
       "\n",
       "   Example:\n",
       "   ```python\n",
       "   my_list = [4, 2, 1, 3]\n",
       "   sorted_list = sorted(my_list)\n",
       "   print(sorted_list)  # Output: [1, 2, 3, 4]\n",
       "   ```\n",
       "\n",
       "Both methods work for lists containing elements of the same or different data types."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt\n",
    "how to sort lists in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48298f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjaydasgupta/.pyenv/versions/pdf_web/lib/python3.10/site-packages/langchain_community/llms/openai.py:1057: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The `dict.update()` method is used to update a dictionary by merging the key-value pairs from another dictionary or an iterable (such as another dictionary, a list of tuples, or any iterable containing key-value pairs).\n",
       "\n",
       "Here's how `dict.update()` works:\n",
       "\n",
       "- Syntax: `dict.update([other])`\n",
       "- `dict` refers to the dictionary on which the method is called.\n",
       "- `other` is an optional argument that can be another dictionary or an iterable containing key-value pairs.\n",
       "\n",
       "When `other` is a dictionary, the `dict.update()` method adds the key-value pairs from `other` into the calling dictionary. If any key already exists in the calling dictionary, its value is updated with the value from `other`.\n",
       "\n",
       "Example 1:\n",
       "```python\n",
       "my_dict = {'a': 1, 'b': 2}\n",
       "other_dict = {'b': 3, 'c': 4}\n",
       "my_dict.update(other_dict)\n",
       "print(my_dict)\n",
       "# Output: {'a': 1, 'b': 3, 'c': 4}\n",
       "```\n",
       "\n",
       "When `other` is an iterable containing key-value pairs, the `dict.update()` method iterates over each key-value pair and adds them to the calling dictionary. If any key in the iterable already exists in the calling dictionary, its value is updated with the new value.\n",
       "\n",
       "Example 2:\n",
       "```python\n",
       "my_dict = {'a': 1, 'b': 2}\n",
       "key_value_pairs = [('b', 3), ('c', 4)]\n",
       "my_dict.update(key_value_pairs)\n",
       "print(my_dict)\n",
       "# Output: {'a': 1, 'b': 3, 'c': 4}\n",
       "```\n",
       "\n",
       "In both cases, the original dictionary is modified. If you want to keep the original dictionary intact and create a new merged dictionary, you can use the assignment operator (`=`) along with `dict.update()`.\n",
       "\n",
       "Example 3:\n",
       "```python\n",
       "my_dict = {'a': 1, 'b': 2}\n",
       "other_dict = {'b': 3, 'c': 4}\n",
       "merged_dict = my_dict.copy()  # Create a copy of the original dictionary\n",
       "merged_dict.update(other_dict)\n",
       "print(my_dict)      # Original dictionary remains unchanged\n",
       "print(merged_dict)  # Merged dictionary with updates\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai gpt-3.5-turbo\n",
    "explain dict.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea0d150",
   "metadata": {},
   "source": [
    "#### Generate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "400e40aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjaydasgupta/.pyenv/versions/pdf_web/lib/python3.10/site-packages/langchain_community/llms/openai.py:1057: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "AI generated code inserted below &#11015;&#65039;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "text/html": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt -f code \n",
    "define a function that calculates the factorial of n. \n",
    "call the function with 10 as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22d529c2-dd2c-4de6-9875-9683a54a735a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3628800"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def factorial(n):\n",
    "    if n == 0 or n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n-1)\n",
    "\n",
    "factorial(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05ce6aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjaydasgupta/.pyenv/versions/pdf_web/lib/python3.10/site-packages/langchain_community/llms/openai.py:1057: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "AI generated code inserted below &#11015;&#65039;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "text/html": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai chatgpt -f code \n",
    "change the function to be a recursive one. \n",
    "call the function with 8 as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4930ca9a-e2b9-4bf7-bd7e-5ce8a55157cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40320\n"
     ]
    }
   ],
   "source": [
    "def factorial(n):\n",
    "    if n <= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n-1)\n",
    "\n",
    "print(factorial(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39346cd0",
   "metadata": {},
   "source": [
    "#### Clear the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc2d9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai chatgpt -r \n",
    "clear the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dd9873",
   "metadata": {},
   "source": [
    "### Interpolation in Jupyter AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813101b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating an error on purpose\n",
    "a, b = 10, '4'\n",
    "a + b "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee0c17",
   "metadata": {},
   "source": [
    "#### Explain the error and give the solution using interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97391b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai chatgpt\n",
    "Explain the following Python error: {Err[16]}. \n",
    "Give me the correct code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a6e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1='Hello Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a13c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ai error chatgpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed58ec05",
   "metadata": {},
   "source": [
    "#### Explaining code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [n**2 for n in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai chatgpt\n",
    "Please explain the code below:\n",
    "{In[20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6aeaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [0, 1, 3, 6, 10, 15, 21, 28, 36, 45, 55, 66, 78, 91, 105]\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai chatgpt\n",
    "Define a function that takes an argument and returns this sequence of numbers: {Out[22]}\n",
    "The function's argument is the number of elements in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8283ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_sequence(n):\n",
    "    sequence = []\n",
    "    for i in range(n):\n",
    "        next_num = i * (i + 1) // 2\n",
    "        sequence.append(next_num)\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e463059",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(number_sequence(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f76b2",
   "metadata": {},
   "source": [
    "### Using Jupyter AI with other models and providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c5bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e2752f",
   "metadata": {},
   "source": [
    "#### Generating images using stable-diffusion of Stability AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dcfb59-a084-41e5-81ac-7db8e5af7a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stabilityai/stable-diffusion-2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8bf193",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai huggingface_hub:stabilityai/stable-diffusion-2-1 --format image\n",
    "photo realistic high definition, 8k image of a futuristic automobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b2b03-adcb-43bf-b891-c64b4dbfe4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ai list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2653790b-a1c7-4ecb-bb74-90341d19971a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjaydasgupta/.pyenv/versions/pdf_web/lib/python3.10/site-packages/langchain_community/llms/openai.py:1057: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Format string contains positional fields",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mai\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopenai-chat:gpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPlease explain code below:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{In[7]}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/pdf_web/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.pyenv/versions/pdf_web/lib/python3.10/site-packages/jupyter_ai_magics/magics.py:620\u001b[0m, in \u001b[0;36mAiMagics.ai\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    617\u001b[0m ip \u001b[38;5;241m=\u001b[39m get_ipython()\n\u001b[1;32m    618\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat_map(FormatDict(ip\u001b[38;5;241m.\u001b[39muser_ns))\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_ai_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/pdf_web/lib/python3.10/site-packages/jupyter_ai_magics/magics.py:555\u001b[0m, in \u001b[0;36mAiMagics.run_ai_cell\u001b[0;34m(self, args, prompt)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;66;03m# interpolate user namespace into prompt\u001b[39;00m\n\u001b[1;32m    554\u001b[0m ip \u001b[38;5;241m=\u001b[39m get_ipython()\n\u001b[0;32m--> 555\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFormatDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_ns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider\u001b[38;5;241m.\u001b[39mis_chat_provider:\n\u001b[1;32m    558\u001b[0m     result \u001b[38;5;241m=\u001b[39m provider\u001b[38;5;241m.\u001b[39mgenerate([[HumanMessage(content\u001b[38;5;241m=\u001b[39mprompt)]])\n",
      "\u001b[0;31mValueError\u001b[0m: Format string contains positional fields"
     ]
    }
   ],
   "source": [
    "%%ai openai-chat:gpt-3.5-turbo\n",
    "Please explain code below:\n",
    "{In[7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ac0e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "def summarize(chain_inputs, text_inputs, prompt_inputs={}):\n",
    "\t\n",
    "\tchain_type = chain_inputs['chain_type']\n",
    "\tif prompt_inputs == {}:\n",
    "\t\tchain = load_summarize_chain(**chain_inputs)\n",
    "\n",
    "\telif chain_type == 'stuff':\n",
    "\t\tprompt = PromptTemplate(**prompt_inputs)\n",
    "\t\tchain_inputs = dict(**{**chain_inputs, **{'prompt': prompt}})\n",
    "\n",
    "\telif chain_type == 'map_reduce':\n",
    "\t\tmap_prompt = PromptTemplate(**prompt_inputs['map'])\n",
    "\t\tcombine_prompt = PromptTemplate(**prompt_inputs['combine'])\n",
    "\t\tchain_inputs = dict(**{**chain_inputs, \n",
    "\t\t\t\t\t\t       **{'map_prompt': map_prompt},\n",
    "\t\t\t\t\t\t       **{'combine_prompt': combine_prompt}\n",
    "\t\t})\n",
    "\n",
    "\telif chain_type == 'refine':\n",
    "\t\tquestion_prompt = PromptTemplate(**prompt_inputs['question'])\n",
    "\t\trefine_prompt = PromptTemplate(**prompt_inputs['refine'])\n",
    "\t\tchain_inputs = dict(**{**chain_inputs, \n",
    "\t\t\t\t\t\t       **{'question_prompt': question_prompt},\n",
    "\t\t\t\t\t\t       **{'refine_prompt': refine_prompt}\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tPrint(\"Chain type not supported. Please use stuff, map_reduce or refine\")\n",
    "\t\treturn\n",
    "\n",
    "\tchain = load_summarize_chain(**chain_inputs)\n",
    "\tsummarized_text = chain.run(text_inputs)\n",
    "\treturn summarized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31cc4a-8d00-4a77-b4a9-f3c9d47360df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
