{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288ce9a8",
   "metadata": {
    "id": "288ce9a8"
   },
   "source": [
    "# LangChain Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d92f1",
   "metadata": {
    "executionInfo": {
     "elapsed": 6656,
     "status": "ok",
     "timestamp": 1695281250556,
     "user": {
      "displayName": "Andrei Dumitrescu",
      "userId": "04285534149796751164"
     },
     "user_tz": -180
    },
    "id": "002d92f1"
   },
   "outputs": [],
   "source": [
    "pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff28727",
   "metadata": {
    "id": "4ff28727"
   },
   "outputs": [],
   "source": [
    "# !pip - installs the packages in the base environment\n",
    "# pip - installs the packages in the virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf2f431",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4913,
     "status": "ok",
     "timestamp": 1695281268475,
     "user": {
      "displayName": "Andrei Dumitrescu",
      "userId": "04285534149796751164"
     },
     "user_tz": -180
    },
    "id": "bdf2f431",
    "outputId": "01809e7f-0539-4eae-b6af-19117ab595eb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.353\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /Users/sanjaydasgupta/.pyenv/versions/3.10.9/envs/pdf_web/lib/python3.10/site-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: langfuse\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14256a6",
   "metadata": {
    "id": "d14256a6",
    "outputId": "dd3ada21-4a44-4f41-a463-579295a371b5"
   },
   "outputs": [],
   "source": [
    "# upgrading langchain\n",
    "!pip install langchain --upgrade -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4407a3a",
   "metadata": {
    "id": "f4407a3a"
   },
   "source": [
    "#### Python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9420415",
   "metadata": {
    "id": "d9420415",
    "outputId": "0e64cc2d-6d74-41b7-a8c2-b311e2b72ebc",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3548c23a-9cab-4e6c-a14c-028194ead3e6'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# loading the API Keys (OpenAI, Pinecone) from .env\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "os.environ.get('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29231e2",
   "metadata": {
    "id": "e29231e2"
   },
   "source": [
    "### LLM Models (Wrappers): GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d37cc56",
   "metadata": {
    "id": "8d37cc56",
    "outputId": "0ce7f40e-3a0b-48f7-9bad-fd5b4fc91abe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.7, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'logit_bias': {}, 'max_tokens': 512}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name='text-davinci-003', temperature=0.7, max_tokens=512)\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d355809",
   "metadata": {
    "id": "5d355809",
    "outputId": "d4362c8b-be35-43e2-81f9-8dfd9f4711b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Quantum mechanics is a theory that describes the behavior of matter and energy at the atomic and subatomic level.\n"
     ]
    }
   ],
   "source": [
    "output = llm('explain quantum mechanics in one sentence')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e29c5e84",
   "metadata": {
    "id": "e29c5e84",
    "outputId": "4029edf6-7c35-4a06-dda7-e6093f356929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(llm.get_num_tokens('explain quantum mechanics in one sentence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00248e0",
   "metadata": {
    "id": "d00248e0"
   },
   "outputs": [],
   "source": [
    "output = llm.generate(['... is the capital of France.',\n",
    "                   'What is the formula for the area of a circle?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b98fd88a",
   "metadata": {
    "id": "b98fd88a",
    "outputId": "9b1d4381-af93-4bf7-98cb-2a992155af78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Generation(text='\\n\\nParis', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nA = Ï€r^2', generation_info={'finish_reason': 'stop', 'logprobs': None})]]\n"
     ]
    }
   ],
   "source": [
    "print(output.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0725e38",
   "metadata": {
    "id": "e0725e38",
    "outputId": "cb53ba44-16ef-4011-a757-bd5770cce2cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Paris\n"
     ]
    }
   ],
   "source": [
    "print(output.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7703b86d",
   "metadata": {
    "id": "7703b86d",
    "outputId": "74a7c223-0098-460c-c333-37a5925b42f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "779b3a6b",
   "metadata": {
    "id": "779b3a6b"
   },
   "outputs": [],
   "source": [
    "output = llm.generate(['Write an orignal tagline for a burger restaurant'] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d3cdfd8",
   "metadata": {
    "id": "1d3cdfd8",
    "outputId": "e8cfd1d7-8557-43c5-dd99-a325528382f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Taste the Juiciest Burgers Around - At Burger Bites!\"\n",
      "\n",
      "\"Come for the burgers, stay for the sizzle!\"\n",
      "\n",
      "\"A Burger to Satisfy Every Craving!\""
     ]
    }
   ],
   "source": [
    "for o in output.generations:\n",
    "    print(o[0].text, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20ee709",
   "metadata": {
    "id": "c20ee709"
   },
   "source": [
    "### ChatModels: GPT-3.5-Turbo and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054eadaa",
   "metadata": {
    "id": "054eadaa"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bdecaa",
   "metadata": {
    "id": "f9bdecaa",
    "outputId": "57873169-df9d-492e-9c54-fb9cf2b21008"
   },
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5, max_tokens=1024)\n",
    "messages = [\n",
    "    SystemMessage(content='You are a physicist and respond only in German.'),\n",
    "    HumanMessage(content='explain quantum mechanics in one sentence')\n",
    "]\n",
    "output = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e06dd",
   "metadata": {
    "id": "761e06dd",
    "outputId": "38f630b1-14d4-4fd5-d39b-70e5f0072d4d"
   },
   "outputs": [],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ad29b",
   "metadata": {
    "id": "6d3ad29b"
   },
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a31ca4b",
   "metadata": {
    "id": "3a31ca4b"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ed8a8",
   "metadata": {
    "id": "612ed8a8",
    "outputId": "7cf93377-faea-47c8-a86e-d23de5591459"
   },
   "outputs": [],
   "source": [
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following {virus} in {language}.'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['virus', 'language'],\n",
    "    template=template\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28accc9",
   "metadata": {
    "id": "b28accc9",
    "outputId": "9a33f6f3-5b1c-4688-b5c9-51f1419ce176"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name='text-davinci-003', temperature=0.7)\n",
    "output = llm(prompt.format(virus='HIV', language='German'))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d9b84",
   "metadata": {
    "id": "025d9b84"
   },
   "source": [
    "### Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e5278",
   "metadata": {
    "id": "fd1e5278"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following {virus} in {language}.'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['virus', 'language'],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "output = chain.run({'virus': 'HSV', 'language': 'french'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42818a",
   "metadata": {
    "id": "be42818a",
    "outputId": "af68df04-3165-4c7c-ef0e-f6a60d03dd28"
   },
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f283cc13",
   "metadata": {
    "id": "f283cc13"
   },
   "source": [
    "### Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68115d",
   "metadata": {
    "id": "9d68115d",
    "outputId": "2aa75fcc-88bb-4972-c9dc-0eec0f0b7d07",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = OpenAI(model_name='text-davinci-003', temperature=0.7, max_tokens=1024)\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=['concept'],\n",
    "    template='''You are an experienced scientist and Python programmer.\n",
    "    Write a function that implements the concept of {concept}.'''\n",
    ")\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt1)\n",
    "\n",
    "\n",
    "llm2 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=['function'],\n",
    "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
    ")\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt2)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "output = overall_chain.run('softmax')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a9c39",
   "metadata": {
    "id": "ed3a9c39"
   },
   "source": [
    "### LangChain Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8284aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_experimental -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631fd9cf",
   "metadata": {
    "id": "631fd9cf"
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e2fe3",
   "metadata": {
    "id": "115e2fe3",
    "outputId": "e7b894b6-3eee-41b4-faf3-171d2aed2f42"
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "# agent_executor.run('Calculate the square root of the factorial of 20 \\\n",
    "# and display it with 4 decimal points')\n",
    "\n",
    "agent_executor.run('what is the answer to 5.1 ** 7.3?')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0af1eb",
   "metadata": {
    "id": "8f0af1eb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
